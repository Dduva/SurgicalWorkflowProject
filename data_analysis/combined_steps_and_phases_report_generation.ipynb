{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aacbaf98-1e13-42e1-ba3c-007cde7b7dd4",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500a5e9-0da6-475d-a814-747425c0ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409bae4d-d1a9-4e0b-acea-6567afc2c5b8",
   "metadata": {},
   "source": [
    "#### Training and validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069faae4-9349-45c8-9950-ca163cec79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shaded_predictions(predictions_df, file_path):\n",
    "    predictions = predictions_df['Predictions']  \n",
    "    actuals = predictions_df['Actuals']  \n",
    "    colors = {\n",
    "        0: 'blue', \n",
    "        1: 'green', \n",
    "        2: 'red', \n",
    "        3: 'skyblue', \n",
    "        4: 'orange', \n",
    "    }\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 2), sharex=True)\n",
    "    # Plot predictions\n",
    "    for i, value in enumerate(predictions):\n",
    "        axs[0].add_patch(Rectangle((i, 0), 1, 1, color=colors.get(value, 'other')))\n",
    "    # Plot actuals\n",
    "    for i, value in enumerate(actuals):\n",
    "        axs[1].add_patch(Rectangle((i, 0), 1, 1, color=colors.get(value, 'other')))\n",
    "    # Setting the limits of the plot and other aesthetic settings\n",
    "    for ax in axs:\n",
    "        ax.set_xlim(0, max(len(predictions), len(actuals)))\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    # Adding a title\n",
    "    axs[0].set_title('Predictions')\n",
    "    axs[1].set_title('Actuals')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c0946-ceb0-4ea5-9c9b-e395250b485a",
   "metadata": {},
   "source": [
    "#### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25908eba-75ba-4516-aa6c-428aab822893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(saving_path, final_results_folder, folder_names, weight_type):\n",
    "    for weight in weight_type:\n",
    "        for folder_name in folder_names:\n",
    "            ### plot for first 2 training videos only\n",
    "            video_ids = [1, 2]\n",
    "            for video_id in video_ids:\n",
    "                filename = f'training_predictions_{weight_type}.csv'\n",
    "                training_predictions_df = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, filename))\n",
    "                predictions_df = training_predictions_df[training_predictions_df['Video_ids'] == video_id]\n",
    "                file_path = os.path.join(saving_path, final_results_folder, folder_name, f'training_shaded_plot_video{video_id}_{weight_type}.png')\n",
    "                plot_shaded_predictions(predictions_df, file_path)\n",
    "        \n",
    "            filename = f'validation_predictions_{weight_type}.csv'\n",
    "            validation_predictions_df = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, filename))\n",
    "            video_ids = validation_predictions_df['Video_ids'].unique()\n",
    "            for video_id in video_ids:\n",
    "                predictions_df = validation_predictions_df[validation_predictions_df['Video_ids'] == video_id]\n",
    "                file_path = os.path.join(saving_path, final_results_folder, folder_name, f'validation_shaded_plot_video{video_id}_{weight_type}.png')\n",
    "                plot_shaded_predictions(predictions_df, file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927a9657-adfb-49ba-9343-55f65f287b6a",
   "metadata": {},
   "source": [
    "#### Confusion matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9731fd65-af10-47ac-b328-a322c0ed26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_confusion_matrix(saving_path, final_results_folder, folder_name_set, weight_type, label_size):\n",
    "    confusion_matrices = numpy.zeros((5, label_size, label_size))\n",
    "    for i, folder_name in enumerate(folder_name_set):\n",
    "        filename = f'predictions_vs_actuals_{weight_type}_model.csv'\n",
    "        df = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, filename))\n",
    "        y_actual = df['Actuals']\n",
    "        y_pred = df['Predictions']\n",
    "        cmatrix = confusion_matrix(y_actual, y_pred, labels=range(label_size))\n",
    "        row_sums = cmatrix.sum(axis=1, keepdims=True)\n",
    "        cmatrix_percentage = cmatrix / row_sums \n",
    "        # cmatrix_percentage *= 100\n",
    "        confusion_matrices[i] += cmatrix_percentage\n",
    "        print(cmatrix_percentage)\n",
    "    mean = np.mean(confusion_matrices, axis=0)*100\n",
    "    std_dev = np.std(confusion_matrices, axis=0)\n",
    "    mean_std_dev = np.asarray([[f'{mean[i, j]:.1f}% ± {std_dev[i, j]:.2f}' for j in range(label_size)] \n",
    "                               for i in range(label_size)])\n",
    "    #plt.figure(figsize=(8, 6))\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.heatmap(mean, annot=mean_std_dev, fmt='', cmap='Blues', cbar=False)\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    if label_size == 5:\n",
    "        labels = ['debulk', 'medial', 'inferior', 'superior', 'lateral']\n",
    "    elif label_size == 2:\n",
    "        labels = ['debulk', 'dissection']\n",
    "    elif label_size == 3:\n",
    "        labels = ['approach', 'debulk and dissection', 'closure']\n",
    "    ax.xaxis.set_ticklabels(labels)\n",
    "    ax.yaxis.set_ticklabels(labels)\n",
    "    # plt.show()\n",
    "    plt.savefig(os.path.join(saving_path, final_results_folder, f'{folder_name_set[0]}_av_confusion_matrix_{weight_type}.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d0a6fc-3355-4c60-b3fe-bb488355a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_average_confusion_matrix(saving_path, final_results_folder, folder_name_sets, weight_type, label_size):\n",
    "    for weight in weight_type:\n",
    "        for folder_name_set in folder_name_sets:\n",
    "            compute_average_confusion_matrix(saving_path, final_results_folder, folder_name_set, weight, label_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec49e62-a84c-4838-97de-9d512b63acce",
   "metadata": {},
   "source": [
    "#### Metrics generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeea809-3508-49db-8795-be61f206058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_metrics(saving_path, final_results_folder, folder_name_set, weight_type, label_size):\n",
    "    step_matrices = numpy.zeros((5, label_size, 3))\n",
    "    overall_matrices =  numpy.zeros((5, 1, 4))\n",
    "    for i, folder_name in enumerate(folder_name_set):\n",
    "        #temporal_smoothing_metrics_filename = f'temporal_smoothing_metrics_{weight_type}.csv'\n",
    "        steps_metrics_filename = f'metrics_results_{weight_type}_model.csv'\n",
    "        overall_results_filename = f'results_df_{weight_type}_model.csv'\n",
    "        #temporal_smoothing_metrics = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, \n",
    "        #                                                          temporal_smoothing_metrics, temporal_smoothing_metrics_filename))\n",
    "        #steps_df = temporal_smoothing_metrics\n",
    "        steps_df = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, steps_metrics_filename))\n",
    "        overall_df = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, overall_results_filename))\n",
    "        print(os.path.join(saving_path, final_results_folder, folder_name, steps_metrics_filename))\n",
    "        overall_df = pandas.read_csv(os.path.join(saving_path, final_results_folder, folder_name, overall_results_filename))\n",
    "        step_matrices[i] += steps_df[['Precision', 'Recall', 'F1']].values\n",
    "        overall_df['Run'] = folder_name.replace('_', ' ')\n",
    "        overall_df = overall_df.rename(columns={'Unnamed: 0': 'Metric'})\n",
    "        overall_df = overall_df.pivot(index='Run', columns='Metric', values='Value')\n",
    "        overall_df = overall_df.reset_index()\n",
    "        overall_matrices[i] += overall_df[['Weighted average F1', 'Overall accuracy', 'Weighted average precision', 'Weighted average recall']].values\n",
    "\n",
    "    steps_mean = np.mean(step_matrices, axis=0)\n",
    "    steps_std_dev = np.std(step_matrices, axis=0)\n",
    "    \n",
    "    # step metrics\n",
    "    steps_results = pd.DataFrame(steps_mean).applymap(\"{:.2f}\".format) + \" ± \" + pd.DataFrame(steps_std_dev).applymap(\"{:.2f}\".format)\n",
    "    steps_results.columns = ['Precision', 'Recall', 'F1']\n",
    "    if label_size == 5:\n",
    "        labels = ['tumour debulking', 'dissection medial', 'dissection inferior', 'dissection superior', 'dissection lateral']\n",
    "    elif label_size == 2:\n",
    "        labels = ['tumour debulking', 'dissection']\n",
    "    elif label_size == 3:\n",
    "        labels = ['approach', 'debulk and dissection', 'closure']\n",
    "    steps_results['Category'] = labels\n",
    "    steps_results = steps_results[['Category', 'Precision', 'Recall', 'F1']]\n",
    "    \n",
    "    steps_latex_output = steps_results.to_latex(index=False, header=['Category', 'Precision', 'Recall', 'F1'])\n",
    "    print(steps_latex_output)\n",
    "    with open(os.path.join(saving_path, final_results_folder, f'{folder_name_set[0]}_avg_steps_metrics_{weight_type}.tex'), 'w') as f:\n",
    "        f.write(steps_latex_output)\n",
    "    \n",
    "    # overall metrics\n",
    "    overall_mean = np.mean(overall_matrices, axis=0)\n",
    "    overall_std_dev = np.std(overall_matrices, axis=0)\n",
    "    overall_results = pd.DataFrame(overall_mean).applymap(\"{:.2f}\".format) + \" ± \" + pd.DataFrame(overall_std_dev).applymap(\"{:.2f}\".format)\n",
    "    columns = ['Weighted average F1', 'Overall accuracy', 'Weighted average precision', 'Weighted average recall']\n",
    "    overall_results.columns = columns\n",
    "    overall_latex_output = overall_results.to_latex(index=False, header=columns)\n",
    "    print(overall_latex_output)\n",
    "    with open(os.path.join(saving_path, final_results_folder, f'{folder_name_set[0]}_avg_overall_metrics_{weight_type}.tex'), 'w') as f:\n",
    "        f.write(overall_latex_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c62d0df-0d90-49c0-80eb-70e249f8049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_average_metrics(saving_path, final_results_folder, folder_name_sets, weight_type, label_size):\n",
    "    for weight in weight_type:\n",
    "        for folder_name_set in folder_name_sets:\n",
    "            create_average_metrics(saving_path, final_results_folder, folder_name_set, weight, label_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5322cac-06e8-425e-8ecd-8fc1e06e4ebb",
   "metadata": {},
   "source": [
    "#### Tensorboard results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2b954-9e80-418a-9e4c-94b41a095f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensorboard_results(results_df, tags, tag_ylim_min_map, tag_ylim_max_map, tag_map, run_map, label_map):\n",
    "    for tag in tags:\n",
    "        fig, ax = plt.subplots()\n",
    "        data = results_df[results_df['Tag'] == tag]\n",
    "        \n",
    "        runs = data['Run'].unique()   \n",
    "        # for run in runs: \n",
    "        #     run_data = data[data['Run'] == run]\n",
    "        #     #run_data = run_data[['epoch','Value']]\n",
    "        #     print(run_data)\n",
    "        #     run_data = run_data.sort_values(by='epoch', ascending=True)\n",
    "        #     print(run_data)\n",
    "        #     print('plotting')\n",
    "        #     ax.plot(run_data['epoch'], run_data['Value'], label=f'{run_map[run]}')\n",
    "        for run in runs: \n",
    "            run_data = data[data['Run'] == run]\n",
    "            # Group by 'epoch' and keep only the row with the max 'Wall time' for each group\n",
    "            idx = run_data.groupby('epoch')['Wall time'].idxmax()  # replace 'Wall time' with your actual column name\n",
    "            run_data = run_data.loc[idx]\n",
    "            \n",
    "            run_data = run_data.sort_values(by='epoch', ascending=True)\n",
    "            ax.plot(run_data['epoch'], run_data['Value'], label=f'{run_map[run]}')\n",
    "            \n",
    "        y_min = tag_ylim_min_map[tag] if tag in tag_ylim_min_map else None\n",
    "        y_max = tag_ylim_max_map[tag] if tag in tag_ylim_max_map else None\n",
    "        if y_min is not None and y_max is not None:\n",
    "            ax.set_ylim(tag_ylim_min_map[tag], tag_ylim_max_map[tag])\n",
    "        # Set the title dynamically based on the tag\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title(f'{tag_map[tag]}')\n",
    "        plt.legend()\n",
    "    \n",
    "        \n",
    "        # Display the plot\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        tag = tag.replace(' ', '_')\n",
    "        plt.savefig(os.path.join(saving_path, final_results_folder, f'{run_map[run]_{tag}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc9b71f-984a-4730-8057-dd8dc7f79bb8",
   "metadata": {},
   "source": [
    "#### All run executions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1005a659-2956-4734-99f2-8a547631e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = '/Users/dorotheeduvaux 1/UCL CSML/MSc Project'\n",
    "final_results_folder = 'FinalResults/extraResults'\n",
    "folder_names = [\n",
    "    'new_network_inv_weights_refl_1fps_250_comb_1',\n",
    "    'new_network_inv_weights_refl_1fps_250_phase_fold1',\n",
    "    'rLSTM_batch30_intprop_ear_1fp250_combined_fold1',\n",
    "    'rLSTM_batch30_intprop_ear_1fp250_phase_fold1'\n",
    "]\n",
    "weight_type = ['best', 'last']\n",
    "\n",
    "## Prediction generation\n",
    "generate_predictions(saving_path, final_results_folder, folder_names, weight_type)\n",
    "\n",
    "## Confusion matrix and metrics for combined steps\n",
    "label_size=2\n",
    "folder_name_sets_combined_steps = [\n",
    "                  ['new_network_inv_weights_refl_1fps_250_comb_1',\n",
    "                  'new_network_inv_weights_refl_1fps_250_comb_2',\n",
    "                  'new_network_inv_weights_refl_1fps_250_comb_3',\n",
    "                  'new_network_inv_weights_refl_1fps_250_comb_4',\n",
    "                  'new_network_inv_weights_refl_1fps_250_comb_5'],\n",
    "                ['rLSTM_batch30_intprop_ear_1fp250_combined_fold1',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_combined_fold2',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_combined_fold3',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_combined_fold4',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_combined_fold5'],\n",
    "]\n",
    "create_average_confusion_matrix(saving_path, final_results_folder, folder_name_sets, weight_type, label_size)\n",
    "create_average_metrics(saving_path, final_results_folder, folder_name_sets, weight_type, label_size)\n",
    "\n",
    "\n",
    "## Confusion matrix and metrics for phases\n",
    "label_size=3\n",
    "folder_name_sets_phases = [\n",
    "                ['rLSTM_batch30_intprop_ear_1fp250_phase_fold1',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_phase_fold2',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_phase_fold3',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_phase_fold4',\n",
    "                'rLSTM_batch30_intprop_ear_1fp250_phase_fold5'],\n",
    "                ['new_network_inv_weights_refl_1fps_250_phase_fold1',\n",
    "                 'new_network_inv_weights_refl_1fps_250_phase_fold2',\n",
    "                 'new_network_inv_weights_refl_1fps_250_phase_fold3',\n",
    "                 'new_network_inv_weights_refl_1fps_250_phase_fold4',\n",
    "                 'new_network_inv_weights_refl_1fps_250_phase_fold5',]\n",
    "]\n",
    "create_average_confusion_matrix(saving_path, final_results_folder, folder_name_sets, weight_type, label_size)\n",
    "create_average_metrics(saving_path, final_results_folder, folder_name_sets, weight_type, label_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd7a9f7-b709-457f-8052-8f7f93920e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tensorboard\n",
    "input_path = '/Users/dorotheeduvaux 1/UCL CSML/MSc Project/FinalResults/tensorboard_data.csv'\n",
    "tensorboard_data_df = pandas.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d1c1d-9457-425c-a6cf-c11ccd8baa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = [\n",
    "    'new_network_inv_weights_refl_1fps_250_comb_1',\n",
    "    'new_network_inv_weights_refl_1fps_250_phase_fold1',\n",
    "    'rLSTM_batch30_intprop_ear_1fp250_combined_fold1',\n",
    "    'rLSTM_batch30_intprop_ear_1fp250_phase_fold1',\n",
    "]\n",
    "saving_path = '/Users/dorotheeduvaux 1/UCL CSML/MSc Project'\n",
    "final_results_folder = 'FinalResults/extraResults'\n",
    "tags = ['training loss epoch ', 'training acc epoch', 'validation loss epoch ', 'validation f1 epoch']\n",
    "tag_ylim_max_map = {\n",
    "          # 'training loss epoch ': 0.65, \n",
    "          #  'training acc epoch': 1,\n",
    "           'validation loss epoch ': 1.5, #3.5, # 4.0,\n",
    "           'validation f1 epoch': 1.5, #0.55\n",
    "          }\n",
    "tag_ylim_min_map = {\n",
    "           # 'training loss epoch ': 0, \n",
    "           # 'training acc epoch': 0.7,\n",
    "           'validation loss epoch ': 0.0, #1.5, # 1.2,\n",
    "           'validation f1 epoch': 0.0, #0.1, #0.3\n",
    "          }\n",
    "tag_map = {'training loss epoch ': 'Training loss', \n",
    "           'training acc epoch': 'Training accuracy',\n",
    "           'validation loss epoch ': 'Validation loss',\n",
    "           'validation f1 epoch': 'Validation F1'\n",
    "          }\n",
    "run_map = {\n",
    "        'new_network_inv_weights_refl_1fps_250_comb_1': 'combined_steps',\n",
    "        'new_network_inv_weights_refl_1fps_250_phase_fold1': 'phases',\n",
    "        'rLSTM_batch30_intprop_ear_1fp250_combined_fold1': 'combined_steps_lstm',\n",
    "        'rLSTM_batch30_intprop_ear_1fp250_phase_fold1': 'phases_lstm',\n",
    "    \n",
    "}\n",
    "\n",
    "tensorboard_data_df['epoch'] = tensorboard_data_df['Step'] + 1\n",
    "tensorboard_data_df['epoch'] = tensorboard_data_df['epoch'].astype(int)\n",
    "results_df = tensorboard_data_df[tensorboard_data_df['Run'].isin(runs)] # & tensorboard_data_df['Tag'].isin(tags)]\n",
    "results_df = results_df.sort_values(by=['Run', 'epoch'])\n",
    "\n",
    "plot_tensorboard_results(results_df, tags, tag_ylim_min_map, tag_ylim_max_map, tag_map, run_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
